{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Category prediction of IKEA furniture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot') # emulate pretty r-style plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File IKEA_SA_Furniture_Web_Scrapings_sss.csv does not exist: 'IKEA_SA_Furniture_Web_Scrapings_sss.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-924b54eb53ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#read the data and delete the index of entry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mIKEA_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'IKEA_SA_Furniture_Web_Scrapings_sss.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mIKEA_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIKEA_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIKEA_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File IKEA_SA_Furniture_Web_Scrapings_sss.csv does not exist: 'IKEA_SA_Furniture_Web_Scrapings_sss.csv'"
     ]
    }
   ],
   "source": [
    "#read the data and delete the index of entry\n",
    "IKEA_df = pd.read_csv('IKEA_SA_Furniture_Web_Scrapings_sss.csv')\n",
    "IKEA_df = IKEA_df.drop(IKEA_df.columns[0],axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IKEA_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m = IKEA_df.shape[0]\n",
    "n = IKEA_df.shape[1]\n",
    "print('We have '+str(m)+' sample sand '+str(n-1)+' features together with the target category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data type of 12 features together with target category are shown as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# data type  \n",
    "IKEA_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some data is missing in feature depth, height and width "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IKEA_df.count() \n",
    "\n",
    "# wouldn't it be prettier if we only print the features for which that is the case instead of all of them?\n",
    "# simply a suggestion...\n",
    "\n",
    "max_count = max(IKEA_df.count())\n",
    "\n",
    "missing_data = []\n",
    "for i in range(len(IKEA_df.count())):\n",
    "        if IKEA_df.count()[i] < max_count:\n",
    "            missing_data.append(i)\n",
    "            \n",
    "IKEA_df.count()[missing_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target and features\n",
    "Now we start to examine the properties of data for each features, which help us better understand the dataset and gives us an ideas of how to clean the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### item_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax0 = IKEA_df.boxplot(column='item_id',by='category',vert=False, figsize=(10,6), fontsize =14,showmeans =True)\n",
    "ax0.set_xlabel('')\n",
    "ax0.figure.savefig('Figures/item_id_boxplot.pdf',bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nun_id =  IKEA_df['item_id'].nunique()\n",
    "print('There are ' + str(nun_id)+' unique item_ids.')\n",
    "if nun_id <  m:\n",
    "    print('item_id is not unique for samples, some samples share same id or might be same one.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The item_id is a number consisting of 5 to 8 digits. In the boxplot above we can see, that approximately 50% of the items of most categories are taking a value between 2e^7 to 7e^7. This means that item_id might does not give strong evidence of category.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nun =  IKEA_df['name'].nunique()\n",
    "#print(IKEA_df['category'].unique())\n",
    "print('There are ' + str(nun) + ' unique name in total' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nun =  IKEA_df['category'].nunique()\n",
    "#print(IKEA_df['category'].unique())\n",
    "print('There are ' + str(nun) + ' categories in total' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "type_df = IKEA_df.groupby('category').size().reset_index(name='size').sort_values(['size'], ascending=False) \n",
    "type_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ax1 = type_df.set_index('category').plot.barh(rot=0, title='Category distribution',color = 'b',figsize=(10,6), fontsize=14)\n",
    "ax1.set_xlabel(\"Number of Samples\")\n",
    "ax1.set_ylabel('')\n",
    "ax1.figure.savefig('Figures/category_distribution.pdf',bbox_inches = 'tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax = type_df.set_index('category').plot.pie(title='Category distribution', y='size',figsize=(10,10), autopct='%1.1f%%',fontsize=12)\n",
    "ax.set_ylabel('')\n",
    "ax.legend(bbox_to_anchor=(2,0.5), loc=\"right\", fontsize=12)\n",
    "ax.figure.savefig('Figures/category_plotpie.pdf',bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph we can observe that more than 16% of the data samples are from the category Tables & desks, followed by Bookcase & sheiving units with approx. 15% and Chairs with approx. 13%. On the other hand, samples of category Room dividers, Sideboards, buffets & console tables, Cafe furniture and Trolleys acount for less than 1%. From this we can conclude, that the classification task itself is quite imbalanced. Hence, it might be necessary to replicate samples of the categories, of which the sample size is extremely small.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax2 = IKEA_df.boxplot(column='price',by='category',vert=False, figsize=(12,6), fontsize =14,showmeans =True)\n",
    "ax2.set_xlabel('Euro')\n",
    "ax2.figure.savefig('Figures/price_boxplot.pdf',bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above in the boxplot of price, the furniture of the category Wardrobes are the most expensive, while the children's furnitures are the cheapest. Sofa & armchairshas has the biggest range in price from about 250€ to 70.000€ while the ourliers can reach up to 90.000€. Cafe furniture has the most stable price. The price area (Interquartile Range of price) of some categories do overlap, but in general it is fair to say that price can be a useful feature in order determine the category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### old_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t = IKEA_df[IKEA_df['old_price'].str.contains('No old price')]['item_id'].count() /m\n",
    "print( str( round(t,4)*100) + '% of feature old_price is no old price' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, it is  impossible to recover this data. It follows, that the feature old_price might not be helpful in order to determine the category. Therefore, it can be ignored for training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sellable_online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = IKEA_df[IKEA_df['sellable_online'] == True]['item_id'].count()/m\n",
    "print( str( round(t,4)*100) + '% of feature sellable_online is True' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indicates, that this feature cannot help to determine the category and therefore can be deleted for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### other_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = IKEA_df[IKEA_df['other_colors'].str.contains('Yes')]['item_id'].count()/m\n",
    "print( str( round(t,4)*100) + '% of feature other_colors is Yes.' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### designer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nun_designer =  IKEA_df['designer'].nunique()\n",
    "#print(IKEA_df['designer'].unique())\n",
    "print('There are ' + str(nun_designer) + ' different designers in total' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "designer_df = IKEA_df.groupby('designer').size().reset_index(name='count').sort_values(['count'], ascending=False)\n",
    "designer_df['count%'] = round(designer_df['count'] *100/m ,2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "designer_df.head(5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The Table above shows that 22% of the item samples are not designed by a sepecific designer. The designer which designs most items is Ehlén Johanssons. However the designer Ehlén Johanssons only acount for about 4% in dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = designer_df[designer_df['count']< 2].shape[0]\n",
    "print(str(t)+ ' of ' + str(nun_designer)+'designers design just 1 items in dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = designer_df[designer_df['count']< 5].shape[0]\n",
    "print(str(t)+ ' of ' + str(nun_designer)+' designers design less than 5 items in dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t  = designer_df[designer_df['count']< 10].shape[0]\n",
    "print(str(t)+ ' of ' + str(nun_designer)+' designers design less than 10 items in dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IKEA_df['designer_len'] = IKEA_df['designer'].str.len() \n",
    "IKEA_df['designer_len'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t = IKEA_df[IKEA_df['designer_len'] > 40].shape[0] \n",
    "print(str( round(t*100/m,2)) +'% samples have actually text of long description in feature designer' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that some entries in desinger is actually a more detailed description of the furniture itself. We could replace them with \"designer unknowm\", however,  this might be misleading to the algorithm as it could be understood as if the furniture was designed by the same person when instead the designer is not known. \n",
    "Moreover we observe that furniture of the same category might have the same text with a long description for the feature **designer**. Considering those mistakes acount for 7% of the whole dataset, we can also just keep the original ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IKEA_df[IKEA_df['designer_len'] > 40].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now examine the favorite furniture category of the TOP3 designers:\n",
    "1. IKEA of Schweden (22.41%)\n",
    "2. Ehlén Johansson (4.36%)\n",
    "3. Francis Cayouette (4.09%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1 = IKEA_df[IKEA_df['designer'].str.contains('IKEA of Sweden')].groupby('category')['item_id'].count() \\\n",
    "                                    .reset_index(name = 'count')\n",
    "axd1 = df1.set_index('category').plot.pie(title='IKEA of Sweden\\'s favor of furniture category', y='count',figsize=(10,10), \\\n",
    "                                          autopct='%1.1f%%', fontsize=12, legend = None)\n",
    "axd1.set_ylabel('')\n",
    "#axd1.legend(bbox_to_anchor=(1.5,0.5), loc=\"right\", fontsize=12)\n",
    "axd1.figure.savefig('Figures/TOP1_designer.pdf',bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IKEA of Sweden covers all 17  furniture catogeries and favor the Bookcases & shelving units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2 = IKEA_df[IKEA_df['designer'].str.contains('Ehlén Johansson')].groupby('category')['item_id'].count() \\\n",
    "                                    .reset_index(name = 'count')\n",
    "axd2 = df2.set_index('category').plot.pie(title='Ehlén Johansson\\'s favor of furniture category', y='count',figsize=(10,10), \\\n",
    "                                          autopct='%1.1f%%', fontsize=12, legend = None)\n",
    "axd2.set_ylabel('')\n",
    "#axd2.legend(bbox_to_anchor=(1.5,0.5), loc=\"right\", fontsize=12)\n",
    "axd2.figure.savefig('Figures/TOP2_designer.pdf',bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t1 = IKEA_df[IKEA_df['category'].str.contains('Wardrobes')]['item_id'].count() \n",
    "t2 = IKEA_df[IKEA_df['category'].str.contains('Wardrobes') & IKEA_df['designer'].str.contains('Ehlén Johansson')]['item_id'].count() \n",
    "print(round(t2/t1, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Designer Ehlén Johansson favors Wardrobes. And indeed, about 73% items of category Wardrobes are designed by Ehlén Johansson (including the cases IKEA of Sweden/Ehlén Johansson and Ehlén Johansson/IKEA of Sweden)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df3 = IKEA_df[IKEA_df['designer'].str.contains('Francis Cayouette')].groupby('category')['item_id'].count() \\\n",
    "                                    .reset_index(name = 'count')\n",
    "axd3 = df3.set_index('category').plot.pie(title='Francis Cayouette\\'s favor of furniture category', y='count',figsize=(10,10),\\\n",
    "                                          autopct='%1.1f%%', fontsize=12, legend = None)\n",
    "axd3.set_ylabel('')\n",
    "#axd3.legend(bbox_to_anchor=(1.5,0.5), loc=\"right\", fontsize=12)\n",
    "axd3.figure.savefig('Figures/TOP3_designer.pdf',bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = IKEA_df[IKEA_df['category'].str.contains('Sofas & armchairs')]['item_id'].count() \n",
    "t2 = IKEA_df[IKEA_df['category'].str.contains('Sofas & armchairs') & IKEA_df['designer'].str.contains('Francis Cayouette')]['item_id'].count() \n",
    "print(round(t2/t1, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Designer Francis Cayouette favors Sofas & armchairs. In total, around 17% items of category Sofas & armchairs are designed by Francis Cayouette."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### depth, height, width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax3 = IKEA_df.boxplot(column='depth',by='category',vert=False, figsize=(10,6), fontsize =14,showmeans =True)\n",
    "ax3.set_xlabel('cm')\n",
    "ax3.figure.savefig('Figures/depth_boxplot.pdf',bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ax4 = IKEA_df.boxplot(column='height',by='category',vert=False, figsize=(10,6), fontsize =14,showmeans =True)\n",
    "ax4.set_xlabel('cm')\n",
    "ax4.figure.savefig('Figures/height_boxplot.pdf',bbox_inches = 'tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax5 = IKEA_df.boxplot(column='width',by='category',vert=False, figsize=(10,6), fontsize =14,showmeans =True)\n",
    "ax5.set_xlabel('cm')\n",
    "ax5.figure.savefig('Figures/width_boxplot.pdf',bbox_inches = 'tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, the feature of size varies a lot for the different categories. This is the case in particular for the feature **depth**. Hence, the feature of size might play an important role in prediction of category. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### link, short_description\n",
    "Feature **link** and **short_desctiption** can be understood literally. Both feature **link** and **short_description** are different for the individual items and uncorrelated to the category. Therefore, **link** and **short_description** will be ignored for the training. However, it should not go unnoted that short description contains size data,  which are not scrapped correctly as feature **depth,height,width**. We can make use of this and recover the NaN in **depth,height,width**. Unfortunately, in most cases this is technically too compilicated. Furthermore, both feature **link** and **short_description** contain key words, which indicate  its category directly. It might be kind of cheating to make use of that, while at the same time it would be extremely difficult to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation\n",
    "We would like to check the colinerity of numerical (including boolean) features. We can encode the nominal features to integer values by using factorization. Although, this could be misleadings in some way, at this point we just want to have an idea of the correlation between features. In case of training an algorithm that does not understand nominal features, we might use the one-hot encoding to avoid misunderstandings caused by numeric values. However, this leads to quite high computational cost. For example, for 370 different designers, one-hot encoding will create 9 more features, respectively, taking value 0/1 to present the feature designer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = IKEA_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df['category'] = pd.factorize(corr_df['category'])[0] + 1\n",
    "corr_df['name'] = pd.factorize(corr_df['name'])[0] + 1\n",
    "corr_df['sellable_online'] = pd.factorize(corr_df['sellable_online'])[0] + 1\n",
    "corr_df['other_colors'] = pd.factorize(corr_df['other_colors'])[0] + 1\n",
    "corr_df['designer'] = pd.factorize(corr_df['designer'])[0] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation / scatter matrix here\n",
    "corr = corr_df.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=np.bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "ax.set_title('Correlation between features')\n",
    "ax.figure.savefig('Figures/corr.pdf',bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the analysis of the individual features above, **depth** and **price** play the relative more important role in determining the **category**, while **sellable_online** and **other_colors** is less correlated to **category**. \n",
    "The price of the items is relative high correlated with depth of items and the name of items is relative high correlated with designer, indicating the colinearity of the features, which might result in bad prediciton in certain algorithms. Anyways, the correlation between features (including the target category) are not at on a high level as all of them are below 0.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert object to boolean\n",
    "IKEA_df['other_colors'] = IKEA_df['other_colors'].map({'Yes': True, 'No': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "IKEA_df[IKEA_df['designer'].str.contains('IKEA of Sweden/Ehlén Johansson')].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "IKEA_df[IKEA_df['designer'].str.contains('Ehlén Johansson/IKEA of Sweden')].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# merging 'IKEA of Sweden/Ehlén Johansson' and 'Ehlén Johansson/IKEA of Sweden'\n",
    "# we noticed when the designers that this is one and the same designer\n",
    "IKEA_df['designer']= np.where(IKEA_df['designer'].str.contains('IKEA of Sweden/Ehlén Johansson'),  'Ehlén Johansson/IKEA of Sweden', IKEA_df.designer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following will let us make the conclusion, that any 'NaN' is in the row containing the data concerning regarding the item-size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# determine where data is missing (cell is NaN)\n",
    "shape_NaN = IKEA_df[IKEA_df.isnull().any(axis=1)].shape \n",
    "IKEA_df[IKEA_df.isnull().any(axis=1)].head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the data is only missing regarding item-size (depth, height, width). Let's check if that is the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "size = ['depth','height','width'] \n",
    "shape_size = IKEA_df[IKEA_df[size].isnull().any(axis=1)].shape \n",
    "if shape_size == shape_NaN:\n",
    "    print('The assumption from above seems to be correct. The only data missing is regarding the item-size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IKEA_df[IKEA_df[IKEA_df.columns[~IKEA_df.columns.isin(size)]].isnull().any(axis=1)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "IKEA_df[IKEA_df[size].isnull().all(axis=1)].head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NaN in size data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the observations made by comparing the given dataset to the product size shown on the website, we came to the conclusion, that there are several main reasons as to why data of the category size is missing \\.\n",
    "First of, it seems that some data is not successfully scraped from the website and not recorded in the short description either making it is impossible to recover the missing data. \\\n",
    "Second, some items represent a set of different furniture. Therefore there is no product size in that sense. \\\n",
    "Third, some items have variable size, for example, an items width width can vary from 190 cm up to 220 cm. Although some data is recorded in the short description, it is very difficult to extract the exact size for all items. The biggest problem is that it cannot be identified what numbers respond to depth, height and width, especially since that data in the short description often is incomplete.\\\n",
    "Last of, there are some small items where the measurements ar taken in milimeters instead of centimeter. Furthermore, for some items a item-diameter was recorded in the short descritpion instead of the item-height.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split description and size data in feature short description\n",
    "IKEA_df['size']= IKEA_df['short_description'].str.rsplit(',', n=1).str[1]\n",
    "IKEA_df['short_description']= IKEA_df['short_description'].str.rsplit(',', n=1).str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split numbers and unit\n",
    "IKEA_df['unit'] = IKEA_df['size'].str.replace('\\d+|-|x', ' ')\n",
    "IKEA_df['size_nounit'] = IKEA_df['size'].str.replace('cm|mm', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following shows that indeed, **height** is not given for the small items. Thus, in this case it is convenient to extract the diameter from the short description representing the diameter and replacing the NaN in height with the recorded data from **short description**. Additionally, we find, that ccording to the link the size-data for small items, of which the diameter is measured with mm, is actually mm as well. Therefore this is corrected in the following.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IKEA_df[IKEA_df['unit'].str.contains('mm') & IKEA_df[size].isnull().any(axis=1)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IKEA_df[IKEA_df['unit'].str.contains('mm') & IKEA_df['height'].isnull()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IKEA_df['height'] = np.where(IKEA_df['unit'].str.contains('mm') , IKEA_df.size_nounit, IKEA_df.height)\n",
    "IKEA_df['height'] = np.where(IKEA_df['unit'].str.contains('mm'), IKEA_df.height.astype(float)/10, IKEA_df.height)\n",
    "IKEA_df['height'] = IKEA_df['height'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IKEA_df['depth'] = np.where(IKEA_df['unit'].str.contains('mm'), IKEA_df.depth.astype(float)/10, IKEA_df.depth)\n",
    "IKEA_df['width'] = np.where(IKEA_df['unit'].str.contains('mm'), IKEA_df.width.astype(float)/10, IKEA_df.width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, the NaN-cells in size-data is filled with either the mean or the median depending on the category and wether there are many outliers in size data (refer to the boxplot earlier). In case there are many outliers resulting in the mean beeing significantly higher than the median, the NaN-cells are filled with the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#calculate median/mean of items of each furniture category \n",
    "dm =IKEA_df.groupby('category')['depth'].median().reset_index(name = 'depth_median')\n",
    "dd = IKEA_df.groupby('category')['depth'].mean().reset_index(name = 'depth_mean')\n",
    "hm = IKEA_df.groupby('category')['height'].mean().reset_index(name = 'height_mean')\n",
    "hd = IKEA_df.groupby('category')['height'].median().reset_index(name = 'height_median')\n",
    "wm= IKEA_df.groupby('category')['width'].median().reset_index(name = 'width_median')\n",
    "wd =IKEA_df.groupby('category')['width'].mean().reset_index(name = 'width_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IKEA_df = IKEA_df.join(dm.set_index('category'), on='category')\n",
    "IKEA_df = IKEA_df.join(dd.set_index('category'), on='category')\n",
    "IKEA_df = IKEA_df.join(hm.set_index('category'), on='category')\n",
    "IKEA_df = IKEA_df.join(hd.set_index('category'), on='category')\n",
    "IKEA_df = IKEA_df.join(wm.set_index('category'), on='category')\n",
    "IKEA_df = IKEA_df.join(wd.set_index('category'), on='category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IKEA_df['depth'] = np.where( (IKEA_df['depth'].isnull()) & (IKEA_df['category'].str.contains('Tables & desks')),  \\\n",
    "                              IKEA_df.depth_median, IKEA_df.depth )\n",
    "IKEA_df['depth'] = np.where( (IKEA_df['depth'].isnull()) & (IKEA_df['category'].str.contains('Sofas & armchairs')),  \\\n",
    "                              IKEA_df.depth_median, IKEA_df.depth )        \n",
    "IKEA_df['depth'] = np.where( (IKEA_df['depth'].isnull()) & (IKEA_df['category'].str.contains('Chairs')),  \\\n",
    "                              IKEA_df.depth_median, IKEA_df.depth )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IKEA_df['height'] = np.where( (IKEA_df['height'].isnull()) & (IKEA_df['category'].str.contains('TV & median furniture')),  \\\n",
    "                              IKEA_df.height_median, IKEA_df.height )\n",
    "IKEA_df['height'] = np.where( (IKEA_df['height'].isnull()) & (IKEA_df['category'].str.contains('Sideboards, buffets,console tables')),  \\\n",
    "                              IKEA_df.height_median, IKEA_df.height )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IKEA_df['width'] = np.where((IKEA_df['width'].isnull()) & (IKEA_df['category'].str.contains('Wardrobes')),  \\\n",
    "                            IKEA_df.width_median, IKEA_df.width)\n",
    "IKEA_df['width'] = np.where((IKEA_df['width'].isnull()) & (IKEA_df['category'].str.contains('Outdoor furniture')),  \\\n",
    "                            IKEA_df.width_median, IKEA_df.width)\n",
    "IKEA_df['width'] = np.where((IKEA_df['width'].isnull()) & (IKEA_df['category'].str.contains('Carbinets & cupboards')),  \\\n",
    "                            IKEA_df.width_median, IKEA_df.width)\n",
    "IKEA_df['width'] = np.where((IKEA_df['width'].isnull()) & (IKEA_df['category'].str.contains('Bookcases & shelving units')),  \\\n",
    "                            IKEA_df.width_median, IKEA_df.width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IKEA_df['depth'] = np.where(IKEA_df['depth'].isnull(), IKEA_df.depth_mean, IKEA_df.depth)\n",
    "IKEA_df['height'] = np.where(IKEA_df['height'].isnull(), IKEA_df.height_mean, IKEA_df.height)\n",
    "IKEA_df['width'] = np.where(IKEA_df['width'].isnull(), IKEA_df.width_mean, IKEA_df.width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data_frame\n",
    "with open('Data/IKEA_df.p', 'wb') as df:   \n",
    "    pickle.dump(IKEA_df, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction with Tree-based algorithm \n",
    "Tree-based algorithms do not work on calculating distance. Therefore it is sufficient to factorize the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = ['item_id','name','category','price','sellable_online','other_colors','designer','depth','height','width' ]\n",
    "learn_df = IKEA_df [learn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#chek if there is still  NaN in dataset\n",
    "learn_df[learn_df.isna().any(axis =1)]['item_id'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#this cell gives warning about setting copy. this warining can be ignored in this project\n",
    "#encode nominal feature into numeric ones\n",
    "learn_df['category_num'] = pd.factorize(learn_df['category'])[0] + 1\n",
    "learn_df['name'] = pd.factorize(learn_df['name'])[0] + 1\n",
    "#learn_df['sellable_online'] = pd.factorize(learn_df['sellable_online'])[0] + 1\n",
    "#learn_df['other_colors'] = pd.factorize(learn_df['other_colors'])[0] + 1\n",
    "learn_df['designer'] = pd.factorize(learn_df['designer'])[0] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label = IKEA_df['category'].unique()\n",
    "label = label.tolist()\n",
    "print('labels:', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = learn_df.drop(columns = ['category','category_num']).shape[1]\n",
    "print('There are '+str(t)+' features.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After modifieying and cleaning the data frame, it is suitable for tre-based algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "**Note**: the exact value of overall accuracy, accuracy in confusion matrix and even ranking of feature importance might slightly differ from the ones written in comments because of randomness in algorithm ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First glance : Standard random forest\n",
    "We first take look at an example calssifier, it is almost a standard random forest classifier apart from that we set min_samples_leaf= 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(learn_df.drop(columns = ['category','category_num']), \\\n",
    "                                                    learn_df['category_num'], random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf =  RandomForestClassifier(min_samples_leaf= 5)\n",
    "rf_est = rf.fit(X_train,y_train)\n",
    "y_pred =  rf.predict(X_test)\n",
    "a = accuracy_score(y_test, y_pred)\n",
    "# Note : this acuracy is not weighted by sample size!!! \n",
    "print('Overall acuracy of all category: ' +str(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** : this overall acuracy is not weighted by sample size , so even for a category with a small size the performance is totally trash, we still have an acceptable overall acuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#check feature importance\n",
    "feature_impo = pd.Series(data=rf_est.feature_importances_, index=list(X_train.columns))\n",
    "feature_impo = feature_impo.sort_values(axis=0, ascending=False)\n",
    "feature_impo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "figip,axip = plt.subplots(1,1, figsize=(8,6))\n",
    "feature_impo.plot(kind='barh', ax=axip)\n",
    "axip.set_xlabel('Importance')\n",
    "axip.set_ylabel('Feature')\n",
    "axip.set_title('Feature importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check the confusion matrix to examine the acuracy for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "cm =  confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# number of smaples actually belong to each category\n",
    "cm_sum = cm.sum(axis = 1)\n",
    "\n",
    "cm1 = np.zeros((17,17))\n",
    "\n",
    "# confusion matrix divided by  number of samples in test set belongs to each category \n",
    "for k in range(17):\n",
    "    cm1[k,:] = cm[k,:]/cm_sum[k]\n",
    "\n",
    "# confusion_matrix normalized \n",
    "cm_df = pd.DataFrame(cm1,index = label, columns=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot confusion matrix \n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "sns.heatmap(cm_df, annot=True, linewidths=.5,ax=ax,fmt='.2f',cmap = cmap)\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('Actual')\n",
    "ax.figure.savefig('Figures/RF_CM_1.pdf', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An analysis of this confusion matrix will be given later; toghether with next plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further feature selection by only keeping the important features\n",
    "From feature importance report we get that size data  play the  great role in prediction, as we asuumed before in feature analysis. In addition name of the items is also important. Lets see what happens if we just keep those 5 top features with importance  larger than around 5%. We remove the feature **sellable online**, **item_id** and **other_colors**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need item_id and category in dataframe for ohther use, not for training\n",
    "learn1 = ['item_id','height','depth','width','name','designer','category','category_num']\n",
    "learn_df1 = learn_df[learn1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(learn_df1.drop(columns = ['item_id','category','category_num']), \\\n",
    "                                                    learn_df1['category_num'], random_state=1)\n",
    "rf =  RandomForestClassifier(min_samples_leaf= 5)\n",
    "rf_est = rf.fit(X_train,y_train)\n",
    "y_pred =  rf.predict(X_test)\n",
    "a = accuracy_score(y_test, y_pred)\n",
    "print('Overall acuracy of all category: ' +str(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall acuracy increased from about 80% to 82%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "cm =  confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# number of smaples actually belong to each category\n",
    "cm_sum = cm.sum(axis = 1)\n",
    "\n",
    "cm1 = np.zeros((17,17))\n",
    "\n",
    "# confusion matrix divided by  number of samples in test set belongs to each category \n",
    "for k in range(17):\n",
    "    cm1[k,:] = cm[k,:]/cm_sum[k]\n",
    "\n",
    "# confusion_matrix normalized \n",
    "cm_df = pd.DataFrame(cm1,index = label, columns=label)\n",
    "\n",
    "#plot confusion matrix \n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "sns.heatmap(cm_df, annot=True, linewidths=.5,ax=ax,fmt='.2f',cmap = cmap)\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('Actual')\n",
    "ax.figure.savefig('Figures/RF_CM_2.pdf', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the both confusion matrix we get that through feature selection,  we increase the overall accuracy at about1%  in the way that the accuracy for category Nursury furniture have been significantly increased by 10% respectly. And it is clear that the category with the worst performance: Room diveider with 0%, Sideboards & console tables with 0%, then Bar furniture 42%. Those are exactly the category with smallest sample size in whole dataset. As we mentioned above we can replicate  those samples to make it to a  ralative balanced multi-class classification task. The category TV & media furniture perform also not good, 30% TV& medianfurnitures have been predicted as Book cases & shelving unit. Considering the feature importance and boxplot of height and depth, we find that those two category have really simillar distribution shown in boxplot of those top 2 features.No wonder then.To our supprise, category Trolleys ,of which samples size is just 28, has the outstanding performance. We did not find the obvious clue from  its size  and price. We take look at the data set and find  the option for its name and designer is restricted and this might be the reason.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replicate the samples for small sample size category  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we replicate the samples of category **Room diveifer** and **Sideboards, buffets & console tables** in dataset to make this multi class classification task be relative balanced and see of it will significantly iprove the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#recall the sample size of each category\n",
    "size_df = learn_df1.groupby('category').size().reset_index(name='size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "size_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# careful here the order of the labels somehow changed compared with list'label' above. The order of TV & median furniture\n",
    "# tables& dsks and trollyes somehow excahnged seeing below . We can't direct use list label to present the correct order of \n",
    "# column names in this section.\n",
    "label1 =size_df['category'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label_size = size_df['size'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_df2 = learn_df1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# using label1 instead of label !!!!!\n",
    "#print(learn_df2.shape[0])\n",
    "for k in range(17):\n",
    "    temp_df = learn_df1[learn_df1['category'].str.contains(label1[k])] \n",
    "    #print('original size' + str(temp_df.shape[0]))\n",
    "    #print('label_size' +str(label_size[k]))\n",
    "    tt = round(max(label_size)/label_size[k]) -1\n",
    "    #print('to replicate' + str(tt))\n",
    "    if tt > 0:\n",
    "        temp_df1 = pd.concat([temp_df]*tt, ignore_index=True)\n",
    "        #print('df_repl size' +str(temp_df1.shape[0]))\n",
    "        learn_df2 = pd.concat([learn_df2, temp_df1] , ignore_index=True)\n",
    "        #print(learn_df2.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# old dataset imbalanced\n",
    "size_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#new relative balanced data set with duplicated samples\n",
    "size_df2 = learn_df2.groupby('category').size().reset_index(name='size')\n",
    "size_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data set has been modified to be a relative balanced ones , although there are now too many duplicated samples for category, which had too few samples before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(learn_df2.drop(columns = ['item_id','category','category_num']), \\\n",
    "                                                    learn_df2['category_num'], random_state=1)\n",
    "rf =  RandomForestClassifier(min_samples_leaf= 5)\n",
    "rf_est = rf.fit(X_train,y_train)\n",
    "y_pred =  rf.predict(X_test)\n",
    "a = accuracy_score(y_test, y_pred)\n",
    "print('Overall acuracy of all category: ' +str(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through replicate samples , we increase the overall accuracy from about 82% to 89%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "cm =  confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# number of smaples actually belong to each category\n",
    "cm_sum = cm.sum(axis = 1)\n",
    "\n",
    "cm1 = np.zeros((17,17))\n",
    "\n",
    "# confusion matrix divided by  number of samples in test set belongs to each category \n",
    "for k in range(17):\n",
    "    cm1[k,:] = cm[k,:]/cm_sum[k]\n",
    "\n",
    "# confusion_matrix normalized \n",
    "cm_df = pd.DataFrame(cm1,index = label, columns=label)\n",
    "\n",
    "#plot confusion matrix \n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "sns.heatmap(cm_df, annot=True, linewidths=.5,ax=ax,fmt='.2f',cmap = cmap)\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('Actual')\n",
    "ax.figure.savefig('Figures/RF_CM_3.pdf', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have an almost perfect diagonal matrix we desired. Through repeating samples to make size of each category  balanced,  the acuracy of prediction for Bar funiture has been increased from 42% to 95% ,  for Room dividers from 0% to 100%,  for Sideboards, buffets & console rable from 0% to 100% etc. It is still hard for the algorithm to distinguish nursery furniture from children furniture.The reason is clear that consering two the immportant features: height and depth, the distribution are almost the same in the box plot.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feature_impo = pd.Series(data=rf_est.feature_importances_, index=list(X_train.columns))\n",
    "feature_impo = feature_impo.sort_values(axis=0, ascending=False)\n",
    "feature_impo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now with replicated samles in data set , the feature name(former) becomes the most important feature, followed by height (former top1) and depth(former top2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now we try out different parameter settings to see if we can get better predictions. After trail and error we keep the following parameter sets to show the impact of those parameters. Best minimum samples in leaf  is  1 (we set 5 before), if increase the size the overall predictions accuracy goes down. Normally the acurracy increase with the size of forest until  reaching certain threshold and the accuracy become stable. In total those parameters just influence the overall acuracy less than 0.01, and the best overall accuracy witih random forest is about 90.27% for minimum_samples_leaf =1, n_tree = 400. Here we won't check the single confusion matrix, since the dataset is now balanced w.r.t sample size of each category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForest(n_tree, min_leaf, learn_df):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(learn_df.drop(columns = ['category_num','category','item_id']), \\\n",
    "                                                        learn_df['category_num'], random_state=1)\n",
    "    print('==================='+'Process for Param_'+str(i)+'_'+ str(l)+'===================')\n",
    "    start_time = time.time()\n",
    "    rf =  RandomForestClassifier(n_estimators = n_tree,criterion = 'gini', min_samples_leaf = min_leaf,random_state =1)\n",
    "    rf.fit(X_train,y_train)\n",
    "    y_pred =  rf.predict(X_test)\n",
    "    end_time = time.time()\n",
    "    print('time consumption: '+ str(end_time - start_time))\n",
    "    # overall acuracy\n",
    "    a = accuracy_score(y_test, y_pred)\n",
    "    print('Overall acuracy: ' +str(a))\n",
    "# uncomment all below to check the confusion matrix for each loop\n",
    "#     # confusion matrix   \n",
    "#     m =  confusion_matrix(y_test, y_pred)\n",
    "#     #calculate total number of samples of each category in X_test\n",
    "#     m_sum = m.sum(axis = 1)\n",
    "#     m1 = np.zeros((17,17))\n",
    "#     for k in range(17):\n",
    "#          m1[k,:] = m[k,:]/m_sum[k]\n",
    "#     m_df = pd.DataFrame(m1,index = label, columns=label)\n",
    "#     #plot confusion matrix\n",
    "#     fig, ax = plt.subplots(figsize=(10,10))\n",
    "#     cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "#     sns.heatmap(m_df, annot=True, linewidths=.5,ax=ax,fmt='.2f',cmap = cmap)\n",
    "#     ax.set_xlabel('Predicted')\n",
    "#     ax.set_ylabel('Actual')\n",
    "#     ax.figure.savefig('Figures/RF_CM_Param_'+str(i)+ '_' + str(l) +'_'+str(round(a,3))+'_.jpg',bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of the  decision tree in forest\n",
    "n_tree = [20,50,100,200,400,600]\n",
    "# those two criterion almost make no difference to overall acuracy,  too complicated to examine their impact on each category\n",
    "# so just use the default gini index\n",
    "#crit = ['gini','entropy']\n",
    "# minimum number of samples in the leaf (end node)\n",
    "min_leaf = [1,3,5,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(n_tree)):\n",
    "        for l in range(len(min_leaf)):\n",
    "            RandomForest(n_tree[i], min_leaf[l],learn_df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
